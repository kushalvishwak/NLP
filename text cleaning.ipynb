{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Cleaning techniques:\n",
    "1. Normilizing text : Case normilization\n",
    "2. Tokenize - taking yhe smallest part of text\n",
    "    word_tokenize() , worldpunct_tokenize() , tweettoknizer , regex_tokenize\n",
    "3. removing stop word and punchation\n",
    "4. stemming and lemmetizing take the words ot is room form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi welcome toi the course of text analytics. text analytics is very important couse'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"Hi welcome toi the course of text analytics. Text analytics is very important couse\"\n",
    "text = txt.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token can be:\n",
    "      1. words or paire of words\n",
    "      2. Sentences\n",
    "      3. Paragrapg\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'Hotel', 'is', 'awesom', ',', \"isn't?\", 'it', \"couldn't\", 'have', 'been', 'a', 'better', 'place', 'than', 'this']\n"
     ]
    }
   ],
   "source": [
    "txt = \"this Hotel is awesom , isn't? \\\n",
    "it couldn't have been a better place than this\"\n",
    "print(txt.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'Hotel', 'is', 'awesom', ',', 'is', \"n't\", '?', 'it', 'could', \"n't\", 'have', 'been', 'a', 'better', 'place', 'than', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'Hotel', 'is', 'awesom', ',', 'isn', \"'\", 't', '?', 'it', 'couldn', \"'\", 't', 'have', 'been', 'a', 'better', 'place', 'than', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LAMO', '#', 'klling', 'it', ',', 'luv', 'mah', 'lyf', 'YOLO', 'LOL', ':', 'D', '<', '3', '@', 'Kaju']\n"
     ]
    }
   ],
   "source": [
    "txt = \"LAMO #klling it ,luv mah lyf YOLO LOL :D <3 @Kaju\"\n",
    "print(wordpunct_tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LAMO', '#klling', 'it', ',', 'luv', 'mah', 'lyf', 'YOLO', 'LOL', ':D', '<3', '@Kaju']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "print(tokenizer.tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Living life like King Size #Chlling #lifeGols #yolo #WanderLust\"\n",
    "x = txt.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#Chlling', '#lifeGols', '#yolo', '#WanderLust']\n"
     ]
    }
   ],
   "source": [
    "txt_=[]\n",
    "for i in x:\n",
    "    if i.startswith(\"#\"):\n",
    "        txt_.append(i)\n",
    "print(txt_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#Chlling', '#lifeGols', '#yolo', '#WanderLust']\n"
     ]
    }
   ],
   "source": [
    "## Exactly all the hashtag\n",
    "print(regexp_tokenize(txt , \"#[\\w]+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chlling', 'lifeGols', 'yolo', 'WanderLust']\n"
     ]
    }
   ],
   "source": [
    "## Extract only the words associted with the Hashtags\n",
    "print(regexp_tokenize(txt , \"#([\\w]+)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I dont care , more'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_nltk = stopwords.words(\"english\")\n",
    "txt = \" I dont care , more\"\n",
    "tok = word_tokenize(txt.lower())\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dont', 'care']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in tok if\\\n",
    "word not in stop_nltk and word not in list(punctuation)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "1. it takes the words to its from\n",
    "2. It rules based technique which just remove the suffixes\n",
    "3. the stemmed words might not be the part of dictionary\n",
    "4. types of stemmers:\n",
    "    1. Porter stemmer -oldest and a legacy  stemmer developed in 1970\n",
    "    2. snowball stemmer - sophisticated stemmer, faster and suports multiple languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer , SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drive'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer_p = PorterStemmer()\n",
    "stemmer_p.stem(\"driving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'is', 'very', 'methodical', 'and', 'orderly', 'in', 'this', 'excuation']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \" he is very methodical and orderly in this Excuation\"\n",
    "tokens = word_tokenize(txt.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'is', 'veri', 'method', 'and', 'orderli', 'in', 'thi', 'excuat']\n"
     ]
    }
   ],
   "source": [
    "print([stemmer_p.stem(word) for word in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'is', 'veri', 'method', 'and', 'order', 'in', 'this', 'excuat']\n"
     ]
    }
   ],
   "source": [
    "stemmer_s = SnowballStemmer(\"english\")\n",
    "print([stemmer_s.stem(word) for word in tokens ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stem the below sentences\n",
    "\n",
    "text = \"Studies studying cries cry this execute orderly university universal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['studi', 'studi', 'cri', 'cri', 'this', 'execut']\n"
     ]
    }
   ],
   "source": [
    "text = \"studying studying cries cry this execute\"\n",
    "stemmer_k = SnowballStemmer(\"english\")\n",
    "tokens=word_tokenize(text)\n",
    "print([stemmer_k.stem(word) for word in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lemmatization\n",
    "\n",
    "1. like stemming Lemitiztion takes the words to root from called as lemma\n",
    "2. It involvesing resolving words to thrie dictornary form\n",
    "3. a lemma of a words is ists  dictionay fron or cannocial form\n",
    "4. lemmitizer in NLTK user WordNET data set which comprice of synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'is', 'very', 'methodical', 'and', 'orderly', 'in', 'this', 'execuation']\n"
     ]
    }
   ],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "txt = \" he is very methodical and orderly in this Execuation\"\n",
    "tokens = word_tokenize(txt.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'is', 'very', 'methodical', 'and', 'orderly', 'in', 'this', 'execuation']\n"
     ]
    }
   ],
   "source": [
    "print([lemm.lemmatize(word) for word in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lemmatize  the below sentences\n",
    "txt1 = \" he is verty methodical and orderly in this execution\"\n",
    "txt2 = \" he is driving and drives the down of the drived vehicle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'is', 'driving', 'and', 'drive', 'the', 'down', 'of', 'the', 'drived', 'vehicle']\n"
     ]
    }
   ],
   "source": [
    "print([lemm.lemmatize(word) for word in word_tokenize(txt2.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'be', 'drive', 'and', 'drive', 'the', 'down', 'of', 'the', 'drive', 'vehicle']\n"
     ]
    }
   ],
   "source": [
    "print([lemm.lemmatize(word , pos=\"v\") for word in word_tokenize(txt2.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
